# AIGC && LLM 

## 算法 && 模型
### llama
|Stars|Repository|Language|Description|Updated|Created|
|:-|:-|:-|:-|:-|:-|
|49837|[facebookresearch/llama](https://github.com/facebookresearch/llama)|Python|Inference code for LLaMA models|2024-02-18T09:30:17Z|2023-02-14T09:29:12Z
|50854|[ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)|C++|Port of Facebook's LLaMA model in C/C++|2024-02-18T09:33:34Z|2023-03-10T18:58:00Z
|37988|[ollama/ollama](https://github.com/ollama/ollama)|Go|Get up and running with Llama 2, Mistral, and other large language models.|2024-02-18T09:40:48Z|2023-06-26T19:39:32Z
|32902|[oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui)|Python|A Gradio web UI for Large Language Models. Supports transformers, GPTQ, AWQ, EXL2, llama.cpp (GGUF), Llama models.|2024-02-18T09:28:27Z|2022-12-21T04:17:37Z
|28166|[run-llama/llama_index](https://github.com/run-llama/llama_index)|Python|LlamaIndex (formerly GPT Index) is a data framework for your LLM applications|2024-02-18T09:22:15Z|2022-11-02T04:24:54Z
|22271|[chatchat-space/Langchain-Chatchat](https://github.com/chatchat-space/Langchain-Chatchat)|Python|Langchain-Chatchat（原Langchain-ChatGLM）基于 Langchain 与 ChatGLM 等语言模型的本地知识库问答 | Langchain-Chatchat (formerly langchain-ChatGLM), local knowledge based LLM (like ChatGLM) QA app with langchain |2024-02-18T09:29:25Z|2023-03-31T12:12:45Z
|17886|[tloen/alpaca-lora](https://github.com/tloen/alpaca-lora)|Jupyter Notebook|Instruct-tune LLaMA on consumer hardware|2024-02-18T08:17:23Z|2023-03-13T21:52:36Z
|16358|[ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)|Python|中文LLaMA&Alpaca大语言模型+本地CPU/GPU训练部署 (Chinese LLaMA & Alpaca LLMs)|2024-02-18T09:40:43Z|2023-03-15T11:09:39Z
|14784|[vllm-project/vllm](https://github.com/vllm-project/vllm)|Python|A high-throughput and memory-efficient inference and serving engine for LLMs|2024-02-18T09:36:57Z|2023-02-09T11:23:20Z
|14256|[karpathy/llama2.c](https://github.com/karpathy/llama2.c)|C|Inference Llama 2 in one file of pure C|2024-02-18T09:17:01Z|2023-07-23T05:15:06Z
|13913|[haotian-liu/LLaVA](https://github.com/haotian-liu/LLaVA)|Python|[NeurIPS'23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond.|2024-02-18T09:28:12Z|2023-04-17T16:13:11Z
|13021|[cocktailpeanut/dalai](https://github.com/cocktailpeanut/dalai)|CSS|The simplest way to run LLaMA on your local machine|2024-02-18T02:41:04Z|2023-03-12T20:07:32Z
|11678|[hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)|Python|Easy-to-use LLM fine-tuning framework (LLaMA, BLOOM, Mistral, Baichuan, Qwen, ChatGLM)|2024-02-18T09:36:54Z|2023-05-28T10:09:12Z
|10992|[PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)|Python|👑 Easy-to-use and powerful NLP and LLM library with 🤗 Awesome model zoo, supporting wide-range of NLP tasks from research to industrial applications, including 🗂Text Classification,  🔍 Neural Search, ❓ Question Answering, ℹ️ Information Extraction, 📄 Document Intelligence, 💌 Sentiment Analysis etc.|2024-02-18T08:15:55Z|2021-02-05T13:07:42Z
|10529|[ludwig-ai/ludwig](https://github.com/ludwig-ai/ludwig)|Python|Low-code framework for building custom LLMs, neural networks, and other AI models|2024-02-18T07:30:01Z|2018-12-27T23:58:12Z
|10012|[getumbrel/llama-gpt](https://github.com/getumbrel/llama-gpt)|TypeScript|A self-hosted, offline, ChatGPT-like chatbot. Powered by Llama 2. 100% private, with no data leaving your device. New: Code Llama support!|2024-02-18T07:19:10Z|2023-07-22T20:12:54Z
|9753|[h2oai/h2ogpt](https://github.com/h2oai/h2ogpt)|Python|Private Q&A and summarization of documents+images or chat with local GPT, 100% private, Apache 2.0. Supports Mixtral, llama.cpp, and more. Demo: https://gpt.h2o.ai/ https://codellama.h2o.ai/|2024-02-18T09:24:28Z|2023-03-24T21:31:25Z
|9392|[Mozilla-Ocho/llamafile](https://github.com/Mozilla-Ocho/llamafile)|C++|Distribute and run LLMs with a single file.|2024-02-18T09:23:51Z|2023-09-10T21:12:32Z
|8395|[bigscience-workshop/petals](https://github.com/bigscience-workshop/petals)|Python|🌸 Run LLMs at home, BitTorrent-style. Fine-tuning and inference up to 10x faster than offloading|2024-02-17T16:35:55Z|2022-06-12T00:10:27Z
|8319|[LlamaFamily/Llama2-Chinese](https://github.com/LlamaFamily/Llama2-Chinese)|Python|Llama中文社区，最好的中文Llama大模型，完全开源可商用|2024-02-18T09:25:31Z|2023-07-19T04:45:23Z
|8155|[bentoml/OpenLLM](https://github.com/bentoml/OpenLLM)|Python|Operating LLMs in production|2024-02-18T07:49:15Z|2023-04-19T00:27:52Z
|8106|[Mintplex-Labs/anything-llm](https://github.com/Mintplex-Labs/anything-llm)|JavaScript|A multi-user ChatGPT for any LLMs, and vector database. Unlimited documents, messages, and storage in one privacy-focused app. Now available as a desktop application!|2024-02-18T08:27:26Z|2023-06-04T02:29:14Z
|7480|[facebookresearch/llama-recipes](https://github.com/facebookresearch/llama-recipes)|Jupyter Notebook|Scripts for fine-tuning Llama2 with composable FSDP & PEFT methods to cover single/multi-node GPUs. Supports default & custom datasets for applications such as summarization & question answering. Supporting a number of candid inference solutions such as HF TGI, VLLM for local or cloud deployment.Demo apps to showcase Llama2 for WhatsApp & Messenger|2024-02-18T07:43:43Z|2023-07-17T07:33:48Z
|7403|[TheR1D/shell_gpt](https://github.com/TheR1D/shell_gpt)|Python|A command-line productivity tool powered by AI large language models like GPT-4, will help you accomplish your tasks faster and more efficiently.|2024-02-18T08:48:57Z|2023-01-18T19:40:11Z
|7263|[LianjiaTech/BELLE](https://github.com/LianjiaTech/BELLE)|HTML|BELLE: Be Everyone's Large Language model Engine（开源中文对话大模型）|2024-02-18T09:10:48Z|2023-03-17T09:44:11Z
|7075|[openlm-research/open_llama](https://github.com/openlm-research/open_llama)|None|OpenLLaMA, a permissively licensed open source reproduction of Meta AI’s LLaMA 7B trained on the RedPajama dataset|2024-02-18T09:37:57Z|2023-04-28T23:18:23Z
|6559|[SJTU-IPADS/PowerInfer](https://github.com/SJTU-IPADS/PowerInfer)|C|High-speed Large Language Model Serving on PCs with Consumer-grade GPUs|2024-02-18T08:51:18Z|2023-12-15T02:24:10Z
|6112|[jzhang38/TinyLlama](https://github.com/jzhang38/TinyLlama)|Python|The TinyLlama project is an open endeavor to pretrain a 1.1B Llama model on 3 trillion tokens.|2024-02-18T09:06:34Z|2023-09-02T08:12:09Z
|6103|[zilliztech/GPTCache](https://github.com/zilliztech/GPTCache)|Python|Semantic cache for LLMs. Fully integrated with LangChain and llama_index. |2024-02-18T09:00:17Z|2023-03-24T05:51:16Z

### 国内开源模型
|模型链接     | 模型描述    |
| --- | --- |
|[Baichuan2](https://github.com/baichuan-inc/Baichuan2)|百川第二代，提供了7B/13B Base和chat的版本|
|[Baichuan](https://github.com/baichuan-inc/baichuan-7B)|百川智能开源7B大模型可商用免费|
|[ziya2](https://huggingface.co/IDEA-CCNL/Ziya2-13B-Base)|基于Llama2训练的ziya2它终于训练完了|
|[ziya](https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-7B-Reward)|IDEA研究院在7B/13B llama上继续预训练+SFT+RM+PPO+HFTT+COHFT+RBRS|
|[Qwen-7B+14B+70B](https://github.com/QwenLM/Qwen-7B)|阿里开源，可商用，通义千文7B,14B,70B Base和chat模型|
|[InternLM2 7B+20B](https://github.com/InternLM/InternLM)|商汤的书生模型2支持200K|
|[Orion-14B-LongChat](https://github.com/OrionStarAI/Orion)|猎户星空多语言模型支持320K|
|[ChatGLM3](https://github.com/THUDM/ChatGLM3)|ChatGLM3发布，支持工具调用等更多功能，不过泛化性有待评估|
|[ChatGLM2](https://github.com/thudm/chatglm2-6b)|32K长文本，FlashAttention+Multi-Query Attenion的显存优化，更强推理能力，哈哈不过很多简单问题也硬要COT，中英平行能力似乎略有下降的ChatGLM2，但是免费商用！|
|[ChatGLM](https://github.com/THUDM/ChatGLM-6B)   | 清华开源的、支持中英双语的对话语言模型，使用了代码训练，指令微调和RLHF。chatglm2支持超长文本，可免费商用啦！|
|[Yuan-2.0](https://github.com/IEIT-Yuan/Yuan-2.0)|浪潮发布Yuan2.0 2B，51B，102B|
|[YI-200K](https://www.modelscope.cn/models/01ai/Yi-6B-200k/summary)|元一智能开源超长200K的6B，34B模型|
|[YI](https://www.modelscope.cn/models/01ai/Yi-34B-Chat/summary)|元一智能开源34B，6B模型|
|[XVERSE-256K](https://modelscope.cn/models/xverse/XVERSE-13B-256K/summary)|元象发布13B免费商用大模型，虽然很长|
|[XVERSE](https://github.com/xverse-ai/XVERSE-65B)|元象发布13B免费商用大模型|
|[DeepSeek-MOE](https://github.com/deepseek-ai/DeepSeek-MoE)|深度求索发布的DeepSeekMoE 16B Base和caht模型|
|[DeepSeek](https://github.com/deepseek-ai/DeepSeek-LLM)|深度求索发布的7B，67B大模型|
|[LLama2-chinese](https://github.com/FlagAlpha/Llama2-Chinese)|没等太久中文预训练微调后的llama2它来了~|
|[YuLan-chat2](https://github.com/RUC-GSAI/YuLan-Chat)|高瓴人工智能基于Llama-2中英双语继续预训练+指令微调/对话微调|
|[BlueLM](https://github.com/vivo-ai-lab/BlueLM)|Vivo人工智能实验室开源大模型|
|[zephyr-7B](https://ollama.ai/library/zephyr)|HuggingFace 团队基于 UltraChat 和 UltraFeedback 训练了 Zephyr-7B 模型|
|[XWin-LM](https://github.com/Xwin-LM/Xwin-LM)|llama2 + SFT + RLHF|
|[Skywork](https://github.com/SkyworkAI/Skywork)|昆仑万维集团·天工团队开源13B大模型可商用|
|[Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)     |   哈工大中文指令微调的LLaMA  |
|[Moss](https://github.com/OpenLMLab/MOSS)   |  为复旦正名！开源了预训练，指令微调的全部数据和模型。可商用 |
|[InternLM](https://github.com/InternLM/InternLM)| 书生浦语在过万亿 token 数据上训练的多语千亿参数基座模型|
|[Aquila2](https://github.com/FlagAI-Open/Aquila2/blob/main/README_CN.md)|智源更新Aquila2模型系列包括全新34B|
|[Aquila](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila)|智源开源7B大模型可商用免费|
|[UltraLM系列](https://github.com/thunlp/UltraChat)|面壁智能开源UltraLM13B，奖励模型UltraRM，和批评模型UltraCM|
|[PandaLLM](https://github.com/dandelionsllm/pandallm)|LLAMA2上中文wiki继续预训练+COIG指令微调|
|[XVERSE](https://github.com/xverse-ai/XVERSE-13B)|据说中文超越llama2的元象开源模型13B模型|
|[BiLLa](https://github.com/Neutralzz/BiLLa)|LLama词表扩充预训练+预训练和任务1比1混合SFT+指令样本SFT三阶段训练|
|[Phoenix](https://github.com/FreedomIntelligence/LLMZoo)|港中文开源凤凰和奇美拉LLM，Bloom基座，40+语言支持|
|[Wombat-7B](https://huggingface.co/GanjinZero/wombat-7b-delta)|达摩院开源无需强化学习使用RRHF对齐的语言模型, alpaca基座|
|[TigerBot](https://github.com/TigerResearch/TigerBot)|虎博开源了7B 180B的模型以及预训练和微调语料|
|[Luotuo](https://github.com/LC1332/Luotuo-Chinese-LLM)   |  中文指令微调的LLaMA，和ChatGLM   |
|[OpenBuddy](https://github.com/OpenBuddy/OpenBuddy)|Llama 多语言对话微调模型|
|[Chinese Vincuna](https://github.com/Facico/Chinese-Vicuna)|LLama 7B基座，使用Belle+Guanaco数据训练|
|[Linly](https://github.com/CVI-SZU/Linly)|Llama 7B基座，使用belle+guanaco+pclue+firefly+CSL+newscommentary等7个指令微调数据集训练|
|[Firefly](https://github.com/yangjianxin1/Firefly)| 中文2.6B模型，提升模型中文写作，古文能力，待开源全部训练代码，当前只有模型|
|[Baize](https://github.com/project-baize/baize-chatbot)    | 使用100k self-chat对话数据微调的LLama    |
|[BELLE](https://github.com/LianjiaTech/BELLE)    |使用ChatGPT生成数据对开源模型进行中文优化  |
|[Chatyuan](https://github.com/search?q=chatyuan&type=repositories)|chatgpt出来后最早的国内开源对话模型，T5架构是下面PromptCLUE的衍生模型|
|[PromptCLUE](https://github.com/clue-ai/PromptCLUE)    | 多任务Prompt语言模型    |
|[PLUG](https://www.alice-mind.com/portal#/)    |   阿里达摩院发布的大模型，提交申请会给下载链接  |
|[CPM2.0](https://baai.ac.cn/)     |  智源发布CPM2.0|
|[GLM](https://github.com/THUDM/GLM-130B) |   清华发布的中英双语130B预训练模型 |
|[BayLing](https://github.com/ictnlp/BayLing)|基于LLama7B/13B，增强的语言对齐的英语/中文大语言模型|

## 工具
|Stars|Repository|Language|Description|Updated|Created|
|:-|:-|:-|:-|:-|:-|
|60909|[nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all.git)| C++ | gpt4all: open-source LLM chatbots that you can run anywhere | 2024-02-08T03:15:27Z | 2023-03-27T18:49:32Z
|16598|[mudler/LocalAI](https://github.com/mudler/LocalAI)|C++|:robot: The free, Open Source OpenAI alternative. Self-hosted, community-driven and local-first. Drop-in replacement for OpenAI running on consumer-grade hardware. No GPU required. Runs ggml, gguf, GPTQ, onnx, TF compatible models: llama, llama2, rwkv, whisper, vicuna, koala, cerebras, falcon, dolly, starcoder, and many others|2024-02-18T09:41:44Z|2023-03-18T22:58:02Z
|5562|[Moonvy/OpenPromptStudio](https://github.com/Moonvy/OpenPromptStudio.git)|Vue|🥣AIGC提示词可视化编辑器|OPS|OpenPromptStudio|2024-02-07T03:25:47Z|2023-03-25T17:25:15Z
|3762|[IDEA-CCNL/Fengshenbang-LM](https://github.com/IDEA-CCNL/Fengshenbang-LM.git)|Python|Fengshenbang-LM(封神榜大模型)是IDEA研究院认知计算与自然语言研究中心主导的大模型开源体系，成为中文AIGC和认知智能的基础设施。|2024-02-06T08:37:34Z|2021-10-28T09:48:27Z
|3756|[phodal/understand-prompt](https://github.com/phodal/understand-prompt.git)|JupyterNotebook|【🔞🔞🔞内含不适合未成年人阅读的图片】基于我擅长的编程、绘画、写作展开的AI探索和总结：StableDiffusion是一种强大的图像生成模型，能够通过对一张图片进行演化来生成新的图片。ChatGPT是一个基于Transformer的语言生成模型，它能够自动为输入的主题生成合适的文章。而GithubCopilot是一个智能编程助手，能够加速日常编程活动。|2024-02-07T07:29:33Z|2023-02-19T01:22:52Z
|2345|[YiVal/YiVal](https://github.com/YiVal/YiVal.git)|Python|YourAutomaticPromptEngineeringAssistantforGenAIApplications|2024-02-07T13:41:15Z|2023-07-15T02:04:35Z
|1729|[PRIS-CV/DemoFusion](https://github.com/PRIS-CV/DemoFusion.git)|JupyterNotebook|Letusdemocratisehigh-resolutiongeneration!(arXiv2023)|2024-02-08T04:22:00Z|2023-10-29T22:31:09Z
|1672|[deepseek-ai/DreamCraft3D](https://github.com/deepseek-ai/DreamCraft3D.git)|Python|[ICLR2024]OfficialimplementationofDreamCraft3D:Hierarchical3DGenerationwithBootstrappedDiffusionPrior|2024-02-08T03:42:18Z|2023-10-23T07:40:20Z
|1579|[casibase/casibase](https://github.com/casibase/casibase.git)|JavaScript|⚡️Open-sourceAILangChain-likeRAG(Retrieval-AugmentedGeneration)knowledgedatabasewithwebUIandEnterpriseSSO⚡️,supportsOpenAI,Azure,LLaMA,GoogleGemini,HuggingFace,Claude,Grok,etc.,chatbotdemo:https://demo.casibase.com,adminUIdemo:https://demo-admin.casibase.com|2024-02-06T18:11:16Z|2020-05-29T02:45:58Z
|1451|[SciSharp/LLamaSharp](https://github.com/SciSharp/LLamaSharp.git)|C#|RunlocalLLaMA/GPTmodeleasilyandfastinC#!🤗It'salsoeasytointegrateLLamaSharpwithsemantic-kernel,unity,WPFandWebApp.|2024-02-08T04:09:35Z|2023-05-09T18:03:21Z
|1351|[One-2-3-45/One-2-3-45](https://github.com/One-2-3-45/One-2-3-45.git)|Python|officialcodeof"One-2-3-45:AnySingleImageto3DMeshin45SecondswithoutPer-ShapeOptimization"|2024-02-08T01:46:50Z|2023-06-28T22:38:29Z
|1309|[SUDO-AI-3D/zero123plus](https://github.com/SUDO-AI-3D/zero123plus.git)|Python|CoderepositoryforZero123++:aSingleImagetoConsistentMulti-viewDiffusionBaseModel.|2024-02-07T11:04:31Z|2023-10-16T23:22:56Z
|1254|[rese1f/StableVideo](https://github.com/rese1f/StableVideo.git)|Python|[ICCV2023]StableVideo:Text-drivenConsistency-awareDiffusionVideoEditing|2024-02-07T06:06:59Z,视频风格切换|2023-02-19T12:48:30Z
|6738|[YaoApp/yao](https://github.com/YaoApp/yao.git)|Go|:rocket:Aperformanceappenginetocreatewebservicesandapplicationsinminutes.SuitableforAI,IoT,IndustrialInternet,ConnectedVehicles,DevOps,Energy,Financeandmanyotheruse-cases.AI时代的网站框架|2024-02-08T05:20:39Z|2021-09-06T09:20:27Z|
|6344|[open-mmlab/mmagic](https://github.com/open-mmlab/mmagic.git)|JupyterNotebook|OpenMMLabMultimodalAdvanced,Generative,andIntelligentCreationToolbox.Unlockthemagic🪄:Generative-AI(AIGC),easy-to-useAPIs,awsome model zoo,diffusionmodels,fortext-to-imagegeneration,image/videorestoration/enhancement,etc.|2024-02-07T21:11:54Z|2019-08-23T13:04:29Z|
|1944|[camenduru/text-generation-webui-colab](https://github.com/camenduru/text-generation-webui-colab)|Jupyter Notebook|A colab gradio web UI for running Large Language Models|2024-02-18T00:37:49Z|2023-04-08T22:04:37Z

## 文档
|Stars|Repository|Language|Description|Updated|Created|
|:-|:-|:-|:-|:-|:-|
|6341|[liaokongVFX/LangChain-Chinese-Getting-Started-Guide](https://github.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide.git)|None|LangChain的中文入门教程|2024-02-07T13:50:53Z|2023-04-07T13:15:12Z
|4737|[EwingYangs/awesome-open-gpt](https://github.com/EwingYangs/awesome-open-gpt.git)|Python|CollectionofOpenSourceProjectsRelatedtoGPT，GPT相关开源项目合集🚀、精选🔥🔥|2024-02-07T14:44:39Z|2023-04-03T13:05:07Z
|1713|[DSXiangLi/DecryptPrompt](https://github.com/DSXiangLi/DecryptPrompt.git)|None|总结Prompt&LLM论文，开源数据&模型，AIGC应用|2024-02-07T10:03:32Z|2023-02-10T14:10:38Z
|1356|[LearnPrompt/LearnPrompt](https://github.com/LearnPrompt/LearnPrompt.git)|JavaScript|永久免费开源的AIGC课程,目前已支持ChatGPT,Midjourney,Runway,StableDiffusion,AI数字人，AI声音&音乐，大模型微调|2024-02-07T13:45:02Z|2023-04-23T06:51:33Z
|1060|[phodal/aigc](https://github.com/phodal/aigc.git)|Rust|《构筑大语言模型应用：应用开发与架构设计》一本关于LLM在真实世界应用的开源电子书，介绍了大语言模型的基础知识和应用，以及如何构建自己的模型。其中包括Prompt的编写、开发和管理，探索最好的大语言模型能带来什么，以及LLM应用开发的模式和架构设计。|2024-02-07T16:51:52Z|2023-06-22T13:42:41Z
|3068|[luban-agi/Awesome-AIGC-Tutorials](https://github.com/luban-agi/Awesome-AIGC-Tutorials.git)|None|CuratedtutorialsandresourcesforLargeLanguageModels,AIPainting,andmore.|2024-02-07T15:02:05Z|2023-08-22T07:22:52Z

### 模型评测
> 大模型评估尚未出现北极星指标，榜单排名往往和实际使用能力存在较大差异，几天没看感觉有的榜单快被玩坏了......

|榜单|结果|
|----|-----|
|[AlpacaEval：LLM-based automatic evaluation ](https://tatsu-lab.github.io/alpaca_eval/)| 开源模型王者vicuna,openchat, wizardlm|
|[Huggingface Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)|MMLU只评估开源模型，Falcon夺冠，在Eleuther AI4个评估集上评估的LLM模型榜单,vicuna夺冠| 
|[https://opencompass.org.cn/](https://opencompass.org.cn/)|上海人工智能实验室推出的开源榜单|
|[Berkley出品大模型排位赛榜有准中文榜单](https://lmsys.org/blog/2023-05-03-arena/)|Elo评分机制，GPT4自然是稳居第一，GPT4>Claude>GPT3.5>Vicuna>others|
|[CMU开源聊天机器人评测应用](https://github.com/zeno-ml/zeno-build)|ChatGPT>Vicuna>others；在对话场景中训练可能很重要|
|[Z-Bench中文真格基金评测](https://github.com/zhenbench/z-bench)|国产中文模型的编程可用性还相对较低，大家水平差不太多，两版ChatGLM提升明显|
|[Chain-of-thought评估](https://github.com/FranxYao/chain-of-thought-hub)|GSM8k, MATH等复杂问题排行榜|
|[InfoQ 大模型综合能力评估](https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651170429&idx=1&sn=b98af3bd14c9f97f1aa07f0f839bb3ec&scene=21#wechat_redirect)|面向中文，ChatGPT>文心一言> Claude>星火|
|[ToolBench: 工具调用评估榜单](https://github.com/OpenBMB/ToolBench)|工具微调模型和ChatGPT进行对比，提供评测脚本|
|[AgentBench: 推理决策评估榜单](https://github.com/THUDM/AgentBench)|清华联合多高校推出不同任务环境，例如购物，家居，操作系统等场景下模型推理决策能力|
|[FlagEval](https://flageval.baai.ac.cn/#/home)|智源出品主观+客观LLM评分榜单|
|[Bird-Bench](https://bird-bench.github.io/)|更贴合真实世界应用的超大数据库，需要领域知识的NL2SQL榜单，模型追赶人类尚有时日|
|[kola](http://103.238.162.37:31622/LeaderBoard)|以世界知识为核心的评价基准，包括已知的百科知识和未知的近90天网络发布内容，评价知识记忆，理解，应用和创造能力|
|[CEVAL](https://cevalbenchmark.com/index.html#home)|中文知识评估，覆盖52个学科，机器评价主要为多项选择|
|[CMMLU](https://github.com/haonan-li/CMMLU)|67个主题中文知识和推理能力评估，多项选择机器评估|
|[LLMEval3](http://llmeval.com/)|复旦推出的知识问答榜单，涵盖大学作业和考题，题库尽可能来自非互联网避免模型作弊|
